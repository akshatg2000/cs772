{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwPBjGFnbK3N"
      },
      "source": [
        "Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "ZUX6akuDbVt7",
        "outputId": "2ff26081-b5f1-4acc-b278-a0baae9450c8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   )\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    102\u001b[0m     ):\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFdUTbJoHVeb"
      },
      "source": [
        "Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfjYqfSBHP7l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5433zKJJq9x"
      },
      "source": [
        "Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXwJhLhjJuYp",
        "outputId": "6e52e2e1-d9c9-4d9e-f3ff-98373c7c1334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('punkt')\n",
        "\n",
        "brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
        "\n",
        "lowered_tagged_sents = [[(word.lower(), tag) for (word, tag) in sent] for sent in brown_tagged_sents]\n",
        "\n",
        "tagset=['.', 'ADJ','ADP', 'ADV', 'CONJ','DET','NOUN','NUM','PRON','PRT','VERB','X']\n",
        "\n",
        "tag_to_ix = {tag: i for i, tag in enumerate(tagset)}\n",
        "# Set the dimensionality of the word vectors\n",
        "dim = 300\n",
        "\n",
        "# Load the pre-trained GloVe model with dimensionality set to `dim`\n",
        "model_wordvec = api.load('glove-wiki-gigaword-' + str(dim))\n",
        "\n",
        "\n",
        "\n",
        "# Get the Brown Corpus sentences with Universal Tagset tags\n",
        "brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
        "\n",
        "# Lowercase the words in the tagged sentences\n",
        "lowered_tagged_sents = [[(word.lower(), tag) for (word, tag) in sent] for sent in brown_tagged_sents]\n",
        "\n",
        "# Convert each word to its word vector\n",
        "unique_tags=set()\n",
        "word_vectors = []\n",
        "for sent in lowered_tagged_sents:\n",
        "    for word, tag in sent:\n",
        "        # Check if the word is in the GloVe vocabulary\n",
        "        if word in model_wordvec.vocab:\n",
        "            # Add the word vector to the sentence vector\n",
        "            word2vec=model_wordvec[word]\n",
        "            word_vectors.append((word2vec,tag_to_ix[tag]))\n",
        "            unique_tags.add(tag)\n",
        "\n",
        "        else:\n",
        "            # If the word is not in the vocabulary, add a random vector with mean 0 and standard deviation 1\n",
        "            word2vec=np.random.normal(0, 1, dim)\n",
        "            word_vectors.append((word2vec,tag_to_ix[tag]))\n",
        "            unique_tags.add(tag)\n",
        "\n",
        "class POS_NN(nn.Module):\n",
        "\n",
        "   def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(POS_NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "   def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the input dimension to the size of the word vectors\n",
        "input_dim = dim\n",
        "\n",
        "# Set the output dimension to the number of tags in the Universal Tagset\n",
        "output_dim = len(tagset)\n",
        "\n",
        "# Set the dimension of the hidden layer\n",
        "hidden_dim = 128\n",
        "\n",
        "# Define the model\n",
        "model = POS_NN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "train_size = int(0.8 * len(word_vectors))\n",
        "train_sents = word_vectors[:train_size]\n",
        "test_sents = word_vectors[train_size:]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_sents, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        # Unpack the batch into separate lists of words and tags\n",
        "        words = batch[0]\n",
        "        tags = batch[1]\n",
        "\n",
        "        # Convert the sentences and tags to PyTorch tensors\n",
        "        word_tensor = torch.tensor(words, dtype=torch.long).float()\n",
        "        tag_tensor = torch.tensor(tags, dtype=torch.long)\n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(word_tensor)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_function(outputs, tag_tensor)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add the loss to the total loss for this epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, avg_loss))\n",
        "\n",
        "torch.save(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "rWkBSSvXgg9w",
        "outputId": "cd8753f6-9b5c-480b-b5b2-2b313fcfb908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2a57e6ba6ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbrown_tagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'universal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlowered_tagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown_tagged_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADJ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ADP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CONJ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DET'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NUM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PRON'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PRT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'VERB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-2a57e6ba6ff8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbrown_tagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'universal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlowered_tagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown_tagged_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADJ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ADP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CONJ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DET'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NUM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PRON'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PRT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'VERB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# Get everything we can from this piece.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# Update the offset table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m                 \u001b[0;34m\"block reader %s() should return list or tuple.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/reader/tagged.py\u001b[0m in \u001b[0;36mread_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mpara\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msent_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 sent = [\n\u001b[0m\u001b[1;32m    294\u001b[0m                     \u001b[0mstr2tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/reader/tagged.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msent_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 sent = [\n\u001b[0;32m--> 294\u001b[0;31m                     \u001b[0mstr2tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTmJI9jLAB8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mFImwZk8JIKn",
        "outputId": "26a40b27-552d-415b-9aea-035f72dfb518"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown.zip/brown/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f48e701bc349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrown_tagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'universal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number of sentences are: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrown_tagged_sents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example of sentence and POS tagging: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrown_tagged_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "\n",
        "brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
        "\n",
        "print(\"number of sentences are: \",len(brown_tagged_sents) )\n",
        "print(\"example of sentence and POS tagging: \",brown_tagged_sents[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBywP1SeKRne"
      },
      "source": [
        "Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8xE1gfFM7fN",
        "outputId": "f4525a85-96dd-49ee-88e8-decc727ac4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57340\n",
            "[('the', 'DET'), ('fulton', 'NOUN'), ('county', 'NOUN'), ('grand', 'ADJ'), ('jury', 'NOUN'), ('said', 'VERB'), ('friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize and lowercase the sentences\n",
        "lowered_tagged_sents = [[(word.lower(), tag) for (word, tag) in sent] for sent in brown_tagged_sents]\n",
        "print(len(lowered_tagged_sents))\n",
        "print(lowered_tagged_sents[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8_iWV-XYrzH"
      },
      "source": [
        "Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagset=['.', 'ADJ','ADP', 'ADV', 'CONJ','DET','NOUN','NUM','PRON','PRT','VERB','X']"
      ],
      "metadata": {
        "id": "SMdyCG53QrrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njUTZJyKI3nI"
      },
      "outputs": [],
      "source": [
        "tag_to_ix = {tag: i for i, tag in enumerate(tagset)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imikWhRBE5eB"
      },
      "outputs": [],
      "source": [
        "# Set the dimensionality of the word vectors\n",
        "dim = 300\n",
        "\n",
        "# Load the pre-trained GloVe model with dimensionality set to `dim`\n",
        "model_wordvec = api.load('glove-wiki-gigaword-' + str(dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roqHb9N9OBAT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get the Brown Corpus sentences with Universal Tagset tags\n",
        "brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
        "\n",
        "# Lowercase the words in the tagged sentences\n",
        "lowered_tagged_sents = [[(word.lower(), tag) for (word, tag) in sent] for sent in brown_tagged_sents]\n",
        "\n",
        "# Convert each word to its word vector\n",
        "unique_tags=set()\n",
        "word_vectors = []\n",
        "for sent in lowered_tagged_sents:\n",
        "    for word, tag in sent:\n",
        "        # Check if the word is in the GloVe vocabulary\n",
        "        if word in model_wordvec.vocab:\n",
        "            # Add the word vector to the sentence vector\n",
        "            word2vec=model_wordvec[word]\n",
        "            word_vectors.append((word2vec,tag_to_ix[tag]))\n",
        "            unique_tags.add(tag)\n",
        "\n",
        "        else:\n",
        "            # If the word is not in the vocabulary, add a random vector with mean 0 and standard deviation 1\n",
        "            word2vec=np.random.normal(0, 1, dim)\n",
        "            word_vectors.append((word2vec,tag_to_ix[tag]))\n",
        "            unique_tags.add(tag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVGrO8_GF8F8"
      },
      "outputs": [],
      "source": [
        "word_vectors[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uh-Cbpy9M3T"
      },
      "source": [
        "Nueral Model Architecture and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_aCfqK79MaK"
      },
      "outputs": [],
      "source": [
        "class POS_NN(nn.Module):\n",
        "\n",
        "   def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(POS_NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "   def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-zCz4XB90fI"
      },
      "source": [
        "Initiating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoiEL0MF_TYv"
      },
      "outputs": [],
      "source": [
        "# Set the input dimension to the size of the word vectors\n",
        "input_dim = dim\n",
        "\n",
        "# Set the output dimension to the number of tags in the Universal Tagset\n",
        "output_dim = len(tagset)\n",
        "\n",
        "# Set the dimension of the hidden layer\n",
        "hidden_dim = 128\n",
        "\n",
        "# Define the model\n",
        "model = POS_NN(input_dim, hidden_dim, output_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM4bGYnR_qoK"
      },
      "source": [
        "Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vuY9Kp6_vKU"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(word_vectors))\n",
        "train_sents = word_vectors[:train_size]\n",
        "test_sents = word_vectors[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFQsux5hK1lv"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrbfAuoeOHvw"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_sents, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        # Unpack the batch into separate lists of words and tags\n",
        "        words = batch[0]\n",
        "        tags = batch[1]\n",
        "\n",
        "        # Convert the sentences and tags to PyTorch tensors\n",
        "        word_tensor = torch.tensor(words, dtype=torch.long).float()\n",
        "        tag_tensor = torch.tensor(tags, dtype=torch.long)\n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(word_tensor)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_function(outputs, tag_tensor)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add the loss to the total loss for this epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, avg_loss))\n",
        "\n",
        "torch.save(model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anSicLSFL5ri"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WOt7wTgMAZ6"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for sentence, tags in test_sents:\n",
        "    # Convert the sentence to a PyTorch tensor\n",
        "    sentence_tensor = torch.tensor(sentence, dtype=torch.float32)\n",
        "        # Convert the tag to a PyTorch tensor\n",
        "    tag_tensor = torch.tensor(tags, dtype=torch.long)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(sentence_tensor)\n",
        "\n",
        "    # Get the predicted tag\n",
        "    _, predicted = torch.max(outputs.data, 0)\n",
        "\n",
        "    # Update the count of correct predictions and the total count\n",
        "    total += 1\n",
        "    if predicted == tag_tensor:\n",
        "        correct += 1\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Accuracy: {:.2f}%'.format(accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giving context\n",
        "\n"
      ],
      "metadata": {
        "id": "IpygDKFuhvsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the dimensionality of the word vectors\n",
        "dim = 300\n",
        "model_wordvec = api.load('glove-wiki-gigaword-' + str(dim))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSf6Sf9vEyTg",
        "outputId": "63329c0d-cb83-41df-ab7f-906029529b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[===============================================---] 95.6% 359.6/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "# Load the pre-trained GloVe model\n",
        "\n",
        "# Get the Brown Corpus sentences with Universal Tagset tags\n",
        "brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
        "\n",
        "# Lowercase the words in the tagged sentences\n",
        "lowered_tagged_sents = [[(word.lower(), tag) for (word, tag) in sent] for sent in brown_tagged_sents]\n",
        "\n",
        "# Set the tagset\n",
        "tagset = ['.', 'ADJ','ADP', 'ADV', 'CONJ','DET','NOUN','NUM','PRON','PRT','VERB','X']\n",
        "tag_to_ix = {tag: i for i, tag in enumerate(tagset)}\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to get the context of a word in a sentence\n",
        "def get_context(sent, index, context_size):\n",
        "    start = max(0, index - context_size)\n",
        "    end = index\n",
        "    context = sent[start:end]\n",
        "    return context\n",
        "\n",
        "# Convert each word to its word vector along with context\n",
        "word_vectors = []\n",
        "unique_tags = set()\n",
        "\n",
        "# Set the context size\n",
        "context_size = 2\n",
        "\n",
        "for sent in lowered_tagged_sents:\n",
        "    for i, (word, tag) in enumerate(sent):\n",
        "        # Get the context of the word in the sentence\n",
        "        context = get_context(sent, i, context_size)\n",
        "        context_vecs = []\n",
        "        for context_word, _ in context:\n",
        "            # Check if the context word is in the GloVe vocabulary\n",
        "            if context_word in model_wordvec.vocab:\n",
        "                context_vecs.append(model_wordvec[context_word])\n",
        "            else:\n",
        "                # If the context word is not in the vocabulary, add a vector of zeros\n",
        "                context_vecs.append(np.zeros(dim))\n",
        "        if len(context_vecs)!= context_size:\n",
        "          extra=(context_size)-len(context_vecs)\n",
        "          context_vecs.insert(0,np.zeros(dim*extra))\n",
        "        # Check if the word is in the GloVe vocabulary\n",
        "        if word in model_wordvec.vocab:\n",
        "            word2vec = model_wordvec[word]\n",
        "            word_vectors.append((np.concatenate(context_vecs + [word2vec]), tag_to_ix[tag]))\n",
        "            unique_tags.add(tag)\n",
        "        else:\n",
        "            # If the word is not in the vocabulary, add a random vector with mean 0 and standard deviation 1\n",
        "            word2vec = np.random.normal(0, 1, dim)\n",
        "            word_vectors.append((np.concatenate(context_vecs + [word2vec]), tag_to_ix[tag]))\n",
        "            unique_tags.add(tag)\n",
        "\n",
        "class POS_NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(POS_NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the input dimension to the size of\n"
      ],
      "metadata": {
        "id": "77wc0Lxno9wY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6ef5a0-1825-4f1f-827c-55c5329f35a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the input dimension to the size of the concatenated word vectors and context vectors\n",
        "input_dim = dim * (context_size + 1)\n",
        "\n",
        "# Set the output dimension to the number of tags in the Universal Tagset\n",
        "output_dim = len(tagset)\n",
        "\n",
        "# Set the dimension of the hidden layer\n",
        "hidden_dim = 128\n",
        "\n",
        "# Define the model\n",
        "model = POS_NN(input_dim, hidden_dim, output_dim)\n",
        "\n"
      ],
      "metadata": {
        "id": "AFB1nCMrv5wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(word_vectors))\n",
        "train_sents = word_vectors[:train_size]\n",
        "test_sents = word_vectors[train_size:]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_sents, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        # Unpack the batch into separate lists of words and tags\n",
        "        words = batch[0]\n",
        "        tags = batch[1]\n",
        "\n",
        "        # Convert the sentences and tags to PyTorch tensors\n",
        "        word_tensor = torch.tensor(words, dtype=torch.long).float()\n",
        "        tag_tensor = torch.tensor(tags, dtype=torch.long)\n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(word_tensor)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_function(outputs, tag_tensor)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add the loss to the total loss for this epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, avg_loss))\n",
        "\n",
        "torch.save(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "s9QYfWWfADQV",
        "outputId": "a7a902d3-90ea-46a0-a4ab-fd8720137280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-ad101de1610d>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  word_tensor = torch.tensor(words, dtype=torch.long).float()\n",
            "<ipython-input-9-ad101de1610d>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tag_tensor = torch.tensor(tags, dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.2389\n",
            "Epoch [2/50], Loss: 1.1875\n",
            "Epoch [3/50], Loss: 1.1707\n",
            "Epoch [4/50], Loss: 1.1632\n",
            "Epoch [5/50], Loss: 1.1530\n",
            "Epoch [6/50], Loss: 1.1477\n",
            "Epoch [7/50], Loss: 1.1427\n",
            "Epoch [8/50], Loss: 1.1389\n",
            "Epoch [9/50], Loss: 1.1306\n",
            "Epoch [10/50], Loss: 1.1277\n",
            "Epoch [11/50], Loss: 1.1230\n",
            "Epoch [12/50], Loss: 1.1204\n",
            "Epoch [13/50], Loss: 1.1188\n",
            "Epoch [14/50], Loss: 1.1161\n",
            "Epoch [15/50], Loss: 1.1141\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ad101de1610d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Add the loss to the total loss for this epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, fbeta_score\n",
        "\n",
        "def evaluate(model, test_sentences):\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "\n",
        "        word  = sentence[0]\n",
        "        target=sentence[1]\n",
        "        word_tensor = torch.tensor(word, dtype=torch.long).float()\n",
        "        tag_tensor = torch.tensor(target, dtype=torch.long)\n",
        "        # print(word_tensor.shape)\n",
        "        # print(tag_tensor.shape)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(word_tensor)\n",
        "\n",
        "            _, predicted = torch.max(output.data, 0)\n",
        "            # print(target,predicted)\n",
        "            y_true.append(target)\n",
        "            y_pred.append(predicted.item())\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "    f0_5 = fbeta_score(y_true, y_pred, beta=0.5, average='weighted')\n",
        "    f2 = fbeta_score(y_true, y_pred, beta=2, average='weighted')\n",
        "    accuracy = sum([y_true[i] == y_pred[i] for i in range(len(y_true))]) / len(y_true)\n",
        "\n",
        "\n",
        "    return precision, recall, f1, f0_5, f2, accuracy"
      ],
      "metadata": {
        "id": "xbCEQLTx5lLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, f0_5, f2, accuracy=evaluate(model,test_sents)"
      ],
      "metadata": {
        "id": "mL7o0Mob_JKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, fbeta_score\n",
        "\n",
        "def evaluate_class(model, test_sentences):\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "\n",
        "        word  = sentence[0]\n",
        "        target=sentence[1]\n",
        "        word_tensor = torch.tensor(word, dtype=torch.long).float()\n",
        "        tag_tensor = torch.tensor(target, dtype=torch.long)\n",
        "        # print(word_tensor.shape)\n",
        "        # print(tag_tensor.shape)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(word_tensor)\n",
        "\n",
        "            _, predicted = torch.max(output.data, 0)\n",
        "            # print(target,predicted)\n",
        "            y_true.append(target)\n",
        "            y_pred.append(predicted.item())\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=1)\n",
        "    print(y_pred,y_true)\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "metadata": {
        "id": "In6wx6L1CBLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, fbeta_score\n",
        "\n",
        "def cm(model, test_sentences):\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "\n",
        "        word  = sentence[0]\n",
        "        target=sentence[1]\n",
        "        word_tensor = torch.tensor(word, dtype=torch.long).float()\n",
        "        tag_tensor = torch.tensor(target, dtype=torch.long)\n",
        "        # print(word_tensor.shape)\n",
        "        # print(tag_tensor.shape)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(word_tensor)\n",
        "\n",
        "            _, predicted = torch.max(output.data, 0)\n",
        "            # print(target,predicted)\n",
        "            y_true.append(target)\n",
        "            y_pred.append(predicted.item())\n",
        "\n",
        "\n",
        "    labels=['.', 'ADJ','ADP', 'ADV', 'CONJ','DET','NOUN','NUM','PRON','PRT','VERB','X']\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Normalize confusion matrix\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Plot confusion matrix heatmap\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "waOP3Ql-GSEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_statstics(precision, recall, f1, f0_5, f2, accuracy,precision_class,recall_class,f1_class):\n",
        "  print(\"Global statistics\")\n",
        "  print(\"\\n\")\n",
        "  print(\"Accuracy: \",accuracy)\n",
        "  print(\"Precision: \",precision)\n",
        "  print(\"Recall: \",recall)\n",
        "  print(\"F Score\")\n",
        "  print(\"f1 score: \",f1)\n",
        "  print(\"f0_5 score: \",f0_5)\n",
        "  print(\"f2_score: \",f2)\n",
        "  print(\"\\n\")\n",
        "  print(\"Class wise stats\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  for i in range(len(precision_class)):\n",
        "    print(\"Tag \",tagset[i])\n",
        "    print(\"Precision: \",precision_class[i])\n",
        "    print(\"Recall: \",recall_class[i])\n",
        "    print(\"f1 score: \",f1_class[i])\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "pHbgT0MyDN1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model metrics"
      ],
      "metadata": {
        "id": "Czbs-BOFGLs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_statstics(precision, recall, f1, f0_5, f2, accuracy,precision_class,recall_class,f1_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiNue0bIDQW6",
        "outputId": "c6bad1fd-2565-44ca-a47d-6da88bdebba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global statistics\n",
            "\n",
            "\n",
            "Accuracy:  0.5844367225143063\n",
            "Precision:  0.5801361729613262\n",
            "Recall:  0.5844367225143063\n",
            "F Score\n",
            "f1 score:  0.560221713725457\n",
            "f0_5 score:  0.561374688233796\n",
            "f2_score:  0.5710183947511898\n",
            "\n",
            "\n",
            "Class wise stats\n",
            "\n",
            "\n",
            "Tag  .\n",
            "Precision:  0.5444976076555024\n",
            "Recall:  0.8903817617698637\n",
            "f1 score:  0.6757512157665728\n",
            "\n",
            "\n",
            "Tag  ADJ\n",
            "Precision:  0.4815889310421781\n",
            "Recall:  0.35215404699738906\n",
            "f1 score:  0.4068243943821285\n",
            "\n",
            "\n",
            "Tag  ADP\n",
            "Precision:  0.5786843171687154\n",
            "Recall:  0.546242774566474\n",
            "f1 score:  0.5619957583575661\n",
            "\n",
            "\n",
            "Tag  ADV\n",
            "Precision:  0.3443223443223443\n",
            "Recall:  0.11273517727306799\n",
            "f1 score:  0.1698571347902197\n",
            "\n",
            "\n",
            "Tag  CONJ\n",
            "Precision:  0.5619694397283531\n",
            "Recall:  0.04439973172367539\n",
            "f1 score:  0.08229736449527598\n",
            "\n",
            "\n",
            "Tag  DET\n",
            "Precision:  0.6468145099832343\n",
            "Recall:  0.6637600688198952\n",
            "f1 score:  0.6551777374657456\n",
            "\n",
            "\n",
            "Tag  NOUN\n",
            "Precision:  0.630555982774531\n",
            "Recall:  0.7513056624518967\n",
            "f1 score:  0.6856551551133039\n",
            "\n",
            "\n",
            "Tag  NUM\n",
            "Precision:  0.3706293706293706\n",
            "Recall:  0.17785234899328858\n",
            "f1 score:  0.2403628117913832\n",
            "\n",
            "\n",
            "Tag  PRON\n",
            "Precision:  0.845693886657742\n",
            "Recall:  0.5398507377656241\n",
            "f1 score:  0.659016621461854\n",
            "\n",
            "\n",
            "Tag  PRT\n",
            "Precision:  0.4992473657802308\n",
            "Recall:  0.3451266042317031\n",
            "f1 score:  0.40812141099261695\n",
            "\n",
            "\n",
            "Tag  VERB\n",
            "Precision:  0.5411914785049113\n",
            "Recall:  0.5113913064442248\n",
            "f1 score:  0.5258695490492601\n",
            "\n",
            "\n",
            "Tag  X\n",
            "Precision:  0.08163265306122448\n",
            "Recall:  0.06217616580310881\n",
            "f1 score:  0.07058823529411765\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "ld8c8VADIiWM",
        "outputId": "837c3a52-d332-480e-aecd-ead347d2c1b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-1d1c11ab1455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-ea0231a4f67b>\u001b[0m in \u001b[0;36mcm\u001b[0;34m(model, test_sentences)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADJ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ADP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CONJ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DET'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NUM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PRON'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PRT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'VERB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Normalize confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-a619200c3cfd>\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(model, test_sentences)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mword\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mword_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}